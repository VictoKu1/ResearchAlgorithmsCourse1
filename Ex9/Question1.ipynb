{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of the Social Aware Assignment of Passengers in Ridesharing\n",
    "Which was described in the article:\n",
    "Levinger, C., Hazon, N., & Azaria, A. (2022). Social Aware Assignment of Passengers in Ridesharing.\n",
    "In Proceedings of the 2022 ACM Conference on Economics and Computation (EC 2022).\n",
    "Short version: http://azariaa.com/Content/Publications/Social_Assignment_SA.pdf\n",
    "Full version: https://github.com/VictoKu1/ResearchAlgorithmsCourse1/blob/main/Article/2022%2C%20Chaya%20Amos%20Noam%2C%20Socially%20aware%20assignment%20of%20passengers%20in%20ride%20sharing.pdf\n",
    "\n",
    "Paper ID: 1862\n",
    "\n",
    "Implementation of match_and_merge\n",
    "algorithm is based on the pseudocode from the article\n",
    "which is written by Victor Kushnir.\n",
    "\"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "def match_and_merge(G: nx.Graph, k: int) -> list:\n",
    "    \"\"\"\n",
    "    An approximation algorithm for any k ≥ 3, provides a solution for the social aware assignment problem with a ratio of 1/(k-1).\n",
    "\n",
    "    As described in the article under the section \"Algorithm 1: Match and Merge\".\n",
    "\n",
    "    Function receives a graph G and a number k, and returns a partition P of G of all matched sets.\n",
    "\n",
    "    The algorithm consists of k - 1 rounds. Each round is composed of a matching phase followed by a merging phase.\n",
    "    Specifically, in round l MnM computes a maximum matching, M_l ⊆ E_l , for G_l (where G_1 = G). In the merging phase, MnM creates a graph\n",
    "    G_(l+1) that includes a unified node for each pair of matched nodes. G_(l+1) also includes all unmatched nodes, along with their\n",
    "    edges to the unified nodes. Clearly, each node in V_l is composed of up-to l nodes\n",
    "    from V_1. Finally, MnM returns the partition, P, of all the matched sets.\n",
    "\n",
    "    :param G: Graph\n",
    "    :param k: Number of passengers\n",
    "    :return: A partition P of G of all matched sets\n",
    "    \n",
    "    Example where G={(v1,v2),(v2,v3),(v3,v4),(v4,v5),(v4,v6)} and k=4:\n",
    "    >>> G = nx.Graph()\n",
    "    >>> list_of_edges = [(1, 2), (2, 3), (3, 4), (4, 5), (4, 6)]\n",
    "    >>> G.add_edges_from(list_of_edges)\n",
    "    >>> k = 4\n",
    "    >>> print(match_and_merge(G, k))\n",
    "    [[1, 2], [3, 4, 5, 6]]\n",
    "    \"\"\"\n",
    "    # Check if k is correct\n",
    "    if G.number_of_nodes() < k:\n",
    "        raise nx.NetworkXError(\"k cannot be greater than the number of nodes in the graph G\")\n",
    "    # If k is negative, raise an error\n",
    "    elif k < 0:\n",
    "        raise nx.NetworkXError(\"k should be 0≤k≤|V(G)|\")\n",
    "    # If k is 0, return an empty list\n",
    "    elif k == 0:\n",
    "        return []\n",
    "    # If k is 1, return a partition of G where each node is a list\n",
    "    elif k == 1:\n",
    "        return [[node] for node in G.nodes()]\n",
    "    # If k is 2, run the maximum matching algorithm on G and return the result\n",
    "    elif k == 2:\n",
    "        return [list(partition) for partition in nx.maximal_matching(G)]\n",
    "    else:\n",
    "        # Implement G_l=(V_l,E_l) using a dictionary which contains a tuple of V_l and E_l\n",
    "        G = {1: G}\n",
    "        # Should contain the maximal matching of G_l\n",
    "        M = {}\n",
    "        # Loop to find the lth maximal matching and put it in G_(l+1)\n",
    "        for l in range(1, k):\n",
    "            # Initialization of the unified nodes list\n",
    "            unified_nodes = []\n",
    "            # Find the maximal matching of G_l\n",
    "            M[l] = list(nx.maximal_matching(G[l]))\n",
    "            # Make sure that G_(l+1) is a empty graph (It was one of the steps of the algorithm in the article)\n",
    "            G[l+1] = nx.Graph()\n",
    "            # Put the nodes of G_l in G_(l+1)\n",
    "            G[l+1].add_nodes_from(tuple(G[l].nodes()))\n",
    "            # For every match in M_l, add a unified node to G_(l+1) so it will be used to find it when needed\n",
    "            for match in M[l]:\n",
    "                # Add the match to the unified nodes dictionary, so it will be easier to find the unified nodes in each round\n",
    "                unified_nodes.append(match)\n",
    "                # Add a unified node to G_(l+1), which is a tuple of the nodes in the match\n",
    "                G[l+1].add_node(match)\n",
    "                # Remove the nodes in the match from G_(l+1)\n",
    "                G[l+1].remove_nodes_from(list(match))\n",
    "            # For every unified node in G_(l+1), add every v_q in G_(l+1) that is connected to it in G_l, add an edge between them in G_(l+1)\n",
    "            for unified_node in unified_nodes:\n",
    "                for v_q in G[l+1].nodes():\n",
    "                    if unified_node != v_q and any(specific_node != v_q and G[l].has_edge(specific_node, v_q) for specific_node in unified_node):\n",
    "                        if not isinstance(v_q, tuple):\n",
    "                            if v_q in unified_node:\n",
    "                                continue\n",
    "                            else:\n",
    "                                G[l+1].add_edge(unified_node, v_q)\n",
    "                        elif all(specific_node in unified_node for specific_node in v_q) or all(specific_node in v_q for specific_node in unified_node):\n",
    "                            continue\n",
    "                        else:\n",
    "                            G[l+1].add_edge(unified_node, v_q)\n",
    "        # Initialization of the partition P and for every unified node (which is a tuple of nodes) in G_k, add it to P\n",
    "        P = [[unified_node] for unified_node in G[k].nodes()]\n",
    "        # For every partition in P, remove all inner tuple brackets\n",
    "        for partition in P:\n",
    "            while any(isinstance(node, tuple) for node in partition):\n",
    "                for node in partition:\n",
    "                    if isinstance(node, tuple):\n",
    "                        partition.remove(node)\n",
    "                        partition.extend(list(node))\n",
    "            partition.sort()\n",
    "        # For every partition in P, sort it\n",
    "        P.sort()\n",
    "    # Return P\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement of match_and_merge algorithm runtime using z\n",
    "import threading\n",
    "def match_and_merge_threading(G: nx.Graph, k: int) -> list:\n",
    "    # Check if k is correct\n",
    "    if G.number_of_nodes() < k:\n",
    "        raise nx.NetworkXError(\"k cannot be greater than the number of nodes in the graph G\")\n",
    "    # If k is negative, raise an error\n",
    "    elif k < 0:\n",
    "        raise nx.NetworkXError(\"k should be 0≤k≤|V(G)|\")\n",
    "    # If k is 0, return an empty list\n",
    "    elif k == 0:\n",
    "        return []\n",
    "    # If k is 1, return a partition of G where each node is a list\n",
    "    elif k == 1:\n",
    "        return [[node] for node in G.nodes()]\n",
    "    # If k is 2, run the maximum matching algorithm on G and return the result\n",
    "    elif k == 2:\n",
    "        return [list(partition) for partition in nx.maximal_matching(G)]\n",
    "    else:\n",
    "        # Implement G_l=(V_l,E_l) using a dictionary which contains a tuple of V_l and E_l\n",
    "        G = {1: G}\n",
    "        # Should contain the maximal matching of G_l\n",
    "        M = {}\n",
    "        # Loop to find the lth maximal matching and put it in G_(l+1)\n",
    "        for l in range(1, k):\n",
    "            # Initialization of the unified nodes list\n",
    "            unified_nodes = []\n",
    "            # Find the maximal matching of G_l\n",
    "            t1 = threading.Thread(target=nx.maximal_matching, args=(G[l],))\n",
    "            t1.start()\n",
    "            M[l] = list(nx.maximal_matching(G[l]))\n",
    "            t1.join()\n",
    "            # Make sure that G_(l+1) is a empty graph (It was one of the steps of the algorithm in the article)\n",
    "            G[l+1] = nx.Graph()\n",
    "            # Put the nodes of G_l in G_(l+1)\n",
    "            G[l+1].add_nodes_from(tuple(G[l].nodes()))\n",
    "            # For every match in M_l, add a unified node to G_(l+1) so it will be used to find it when needed\n",
    "            for match in M[l]:\n",
    "                # Add the match to the unified nodes dictionary, so it will be easier to find the unified nodes in each round\n",
    "                unified_nodes.append(match)\n",
    "                # Add a unified node to G_(l+1), which is a tuple of the nodes in the match\n",
    "                G[l+1].add_node(match)\n",
    "                # Remove the nodes in the match from G_(l+1)\n",
    "                G[l+1].remove_nodes_from(list(match))\n",
    "            # For every unified node in G_(l+1), add every v_q in G_(l+1) that is connected to it in G_l, add an edge between them in G_(l+1)\n",
    "            for unified_node in unified_nodes:\n",
    "                for v_q in G[l+1].nodes():\n",
    "                    if unified_node != v_q and any(specific_node != v_q and G[l].has_edge(specific_node, v_q) for specific_node in unified_node):\n",
    "                        if not isinstance(v_q, tuple):\n",
    "                            if v_q in unified_node:\n",
    "                                continue\n",
    "                            else:\n",
    "                                G[l+1].add_edge(unified_node, v_q)\n",
    "                        elif all(specific_node in unified_node for specific_node in v_q) or all(specific_node in v_q for specific_node in unified_node):\n",
    "                            continue\n",
    "                        else:\n",
    "                            G[l+1].add_edge(unified_node, v_q)\n",
    "        # Initialization of the partition P and for every unified node (which is a tuple of nodes) in G_k, add it to P\n",
    "        P = [[unified_node] for unified_node in G[k].nodes()]\n",
    "        # For every partition in P, remove all inner tuple brackets using threading\n",
    "        class RemoveTupleBrackets(threading.Thread):\n",
    "            def __init__(self, partition):\n",
    "                threading.Thread.__init__(self)\n",
    "                self.partition = partition\n",
    "            def run(self):\n",
    "                while any(isinstance(node, tuple) for node in self.partition):\n",
    "                    for node in self.partition:\n",
    "                        if isinstance(node, tuple):\n",
    "                            self.partition.remove(node)\n",
    "                            self.partition.extend(list(node))\n",
    "                self.partition.sort()\n",
    "        threads = []\n",
    "        for partition in P:\n",
    "            thread = RemoveTupleBrackets(partition)\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        # For every partition in P, sort it\n",
    "        P.sort()\n",
    "    # Return P\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import networkx as nx\n",
    "def match_and_merge_cython(G, k):\n",
    "    # Check if k is correct\n",
    "    if G.number_of_nodes() < k:\n",
    "        raise nx.NetworkXError(\"k cannot be greater than the number of nodes in the graph G\")\n",
    "    # If k is negative, raise an error\n",
    "    elif k < 0:\n",
    "        raise nx.NetworkXError(\"k should be 0≤k≤|V(G)|\")\n",
    "    # If k is 0, return an empty list\n",
    "    elif k == 0:\n",
    "        return []\n",
    "    # If k is 1, return a partition of G where each node is a list\n",
    "    elif k == 1:\n",
    "        return [[node] for node in G.nodes()]\n",
    "    # If k is 2, run the maximum matching algorithm on G and return the result\n",
    "    elif k == 2:\n",
    "        return [list(partition) for partition in nx.maximal_matching(G)]\n",
    "    else:\n",
    "        # Implement G_l=(V_l,E_l) using a dictionary which contains a tuple of V_l and E_l\n",
    "        G = {1: G}\n",
    "        # Should contain the maximal matching of G_l\n",
    "        M = {}\n",
    "        # Loop to find the lth maximal matching and put it in G_(l+1)\n",
    "        for l in range(1, k):\n",
    "            # Initialization of the unified nodes list\n",
    "            unified_nodes = []\n",
    "            # Find the maximal matching of G_l\n",
    "            M[l] = list(nx.maximal_matching(G[l]))\n",
    "            # Make sure that G_(l+1) is a empty graph (It was one of the steps of the algorithm in the article)\n",
    "            G[l+1] = nx.Graph()\n",
    "            # Put the nodes of G_l in G_(l+1)\n",
    "            G[l+1].add_nodes_from(tuple(G[l].nodes()))\n",
    "            # For every match in M_l, add a unified node to G_(l+1) so it will be used to find it when needed\n",
    "            for match in M[l]:\n",
    "                # Add the match to the unified nodes dictionary, so it will be easier to find the unified nodes in each round\n",
    "                unified_nodes.append(match)\n",
    "                # Add a unified node to G_(l+1), which is a tuple of the nodes in the match\n",
    "                G[l+1].add_node(match)\n",
    "                # Remove the nodes in the match from G_(l+1)\n",
    "                G[l+1].remove_nodes_from(list(match))\n",
    "            # For every unified node in G_(l+1), add every v_q in G_(l+1) that is connected to it in G_l, add an edge between them in G_(l+1)\n",
    "            for unified_node in unified_nodes:\n",
    "                for v_q in G[l+1].nodes():\n",
    "                    if unified_node != v_q and any(specific_node != v_q and G[l].has_edge(specific_node, v_q) for specific_node in unified_node):\n",
    "                        if not isinstance(v_q, tuple):\n",
    "                            if v_q in unified_node:\n",
    "                                continue\n",
    "                            else:\n",
    "                                G[l+1].add_edge(unified_node, v_q)\n",
    "                        elif all(specific_node in unified_node for specific_node in v_q) or all(specific_node in v_q for specific_node in unified_node):\n",
    "                            continue\n",
    "                        else:\n",
    "                            G[l+1].add_edge(unified_node, v_q)\n",
    "        # Initialization of the partition P and for every unified node (which is a tuple of nodes) in G_k, add it to P\n",
    "        P = [[unified_node] for unified_node in G[k].nodes()]\n",
    "        # For every partition in P, remove all inner tuple brackets\n",
    "        for partition in P:\n",
    "            while any(isinstance(node, tuple) for node in partition):\n",
    "                for node in partition:\n",
    "                    if isinstance(node, tuple):\n",
    "                        partition.remove(node)\n",
    "                        partition.extend(list(node))\n",
    "            partition.sort()\n",
    "        # For every partition in P, sort it\n",
    "        P.sort()\n",
    "    # Return P\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Graph' has no attribute 'add_edges'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m results\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n\u001b[1;32m     16\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m threading_results_answer\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m(match_and_merge_threading(G, k)))\n\u001b[1;32m     18\u001b[0m threading_results\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time)\n\u001b[1;32m     19\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [43], line 47\u001b[0m, in \u001b[0;36mmatch_and_merge_threading\u001b[0;34m(G, k)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[39m# For every unified node in G_(l+1), add every v_q in G_(l+1) that is connected to it in G_l, add an edge between them in G_(l+1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[39m# Use multiprocessing to speed up the algorithm\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39mwith\u001b[39;00m mp\u001b[39m.\u001b[39mPool(mp\u001b[39m.\u001b[39mcpu_count()) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m---> 47\u001b[0m         pool\u001b[39m.\u001b[39mmap(partial(nx\u001b[39m.\u001b[39;49mGraph\u001b[39m.\u001b[39;49madd_edges, G\u001b[39m=\u001b[39mG, l\u001b[39m=\u001b[39ml, unified_nodes\u001b[39m=\u001b[39munified_nodes), G[l\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mnodes())\n\u001b[1;32m     48\u001b[0m \u001b[39m# Initialization of the partition P and for every unified node (which is a tuple of nodes) in G_k, add it to P\u001b[39;00m\n\u001b[1;32m     49\u001b[0m P \u001b[39m=\u001b[39m [[unified_node] \u001b[39mfor\u001b[39;00m unified_node \u001b[39min\u001b[39;00m G[k]\u001b[39m.\u001b[39mnodes()]\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Graph' has no attribute 'add_edges'"
     ]
    }
   ],
   "source": [
    "# Comparison of the runtimes of the algorithms\n",
    "import time\n",
    "results = []\n",
    "results_answer = []\n",
    "cython_results = []\n",
    "cython_results_answer = []\n",
    "threading_results = []\n",
    "threading_results_answer = []\n",
    "for k in range(2,25):\n",
    "    for v in range(k, 100):\n",
    "        for p in range(v, 100):\n",
    "            G = nx.gnm_random_graph(v, p)\n",
    "            start_time = time.time()\n",
    "            results_answer.append(len(match_and_merge(G, k)))\n",
    "            results.append(time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            threading_results_answer.append(len(match_and_merge_threading(G, k)))\n",
    "            threading_results.append(time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            cython_results_answer.append(len(match_and_merge_cython(G, k)))\n",
    "            cython_results.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "names = [\"Python\", \"Cython\", \"Threading\"]\n",
    "colors = [\"red\", \"orange\", \"blue\", \"grey\", \"brown\", \"purple\"]\n",
    "info = [results, cython_results, threading_results]\n",
    "answer = [results_answer, cython_results_answer, threading_results_answer]\n",
    "\n",
    "print(\"Runtime:\")\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(info[i], label=names[i], color=colors[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Answer:\")\n",
    "\n",
    "c=0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i!=j:\n",
    "            plt.plot([abs(answer[i][k]-answer[j][k]) for k in range(len(answer[i]))], label=names[i]+\"-\"+names[j], color=colors[c])\n",
    "            c+=1\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
